// Lexer Tests

import { Lexer, Token, TokenKind } from "../core/compiler/frontend/lexer"
import { Error, ErrorKind } from "../core/compiler/error"

#[test]
fn test_basic_tokens() -> Result<(), Error> {
    let source = "let x = 42"
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    assert_eq!(tokens.len(), 4)
    assert_eq!(tokens[0].kind, TokenKind::Let)
    assert_eq!(tokens[1].kind, TokenKind::Identifier)
    assert_eq!(tokens[2].kind, TokenKind::Equals)
    assert_eq!(tokens[3].kind, TokenKind::Integer)
    
    Ok(())
}

#[test]
fn test_string_literals() -> Result<(), Error> {
    let source = r#"let msg = "Hello, World!""#
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    assert_eq!(tokens.len(), 4)
    assert_eq!(tokens[3].kind, TokenKind::String)
    assert_eq!(tokens[3].value, "Hello, World!")
    
    Ok(())
}

#[test]
fn test_indentation() -> Result<(), Error> {
    let source = "
    fn main() {
        let x = 42
    }
    "
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    // Check indentation tokens are correctly generated
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Indent))
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Dedent))
    
    Ok(())
}

#[test]
#[should_panic]
fn test_invalid_character() {
    let source = "let x = @"
    let lexer = Lexer::new(source)
    let _ = lexer.tokenize().unwrap()
}

#[test]
fn test_complex_tokens() -> Result<(), Error> {
    let source = "
    fn calculate(x: i32) -> i32 {
        let result = x * (2 + 3)
        return result
    }
    "
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    // Verify function declaration tokens
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Function))
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Identifier))
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Colon))
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Arrow))
    
    // Verify expression tokens
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Multiply))
    assert!(tokens.iter().any(|t| t.kind == TokenKind::Plus))
    assert!(tokens.iter().any(|t| t.kind == TokenKind::LeftParen))
    assert!(tokens.iter().any(|t| t.kind == TokenKind::RightParen))
    
    Ok(())
}

#[test]
#[property_test]
fn test_identifier_properties(input: String) -> Result<(), Error> {
    // Property: All valid identifiers should be tokenized as identifiers
    if input.chars().all(|c| c.is_alphanumeric() || c == '_') && 
       !input.chars().next().unwrap().is_numeric() {
        let lexer = Lexer::new(&input)
        let tokens = lexer.tokenize()?
        
        assert_eq!(tokens.len(), 1)
        assert_eq!(tokens[0].kind, TokenKind::Identifier)
        assert_eq!(tokens[0].value, input)
    }
    
    Ok(())
}

#[test]
#[fuzz_test]
fn test_lexer_fuzzing(data: &[u8]) {
    if let Ok(input) = String::from_utf8(data.to_vec()) {
        let lexer = Lexer::new(&input)
        let _ = lexer.tokenize(); // Should not crash
    }
}

#[test]
#[benchmark]
fn bench_lexer_performance() -> Result<(), Error> {
    let source = include_str!("../examples/large_file.seo")
    let lexer = Lexer::new(source)
    let _ = lexer.tokenize()?
    Ok(())
}

#[test]
fn test_comments() -> Result<(), Error> {
    let source = "
    // Line comment
    let x = 42 /* Block comment */
    "
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    // Comments should be ignored in token stream
    assert_eq!(tokens.len(), 4)
    assert_eq!(tokens[0].kind, TokenKind::Let)
    assert_eq!(tokens[1].kind, TokenKind::Identifier)
    assert_eq!(tokens[2].kind, TokenKind::Equals)
    assert_eq!(tokens[3].kind, TokenKind::Integer)
    
    Ok(())
}

#[test]
fn test_operators() -> Result<(), Error> {
    let source = "+ - * / % == != < <= > >= && || !"
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    assert_eq!(tokens[0].kind, TokenKind::Plus)
    assert_eq!(tokens[1].kind, TokenKind::Minus)
    assert_eq!(tokens[2].kind, TokenKind::Multiply)
    assert_eq!(tokens[3].kind, TokenKind::Divide)
    assert_eq!(tokens[4].kind, TokenKind::Modulo)
    assert_eq!(tokens[5].kind, TokenKind::EqualsEquals)
    assert_eq!(tokens[6].kind, TokenKind::NotEquals)
    assert_eq!(tokens[7].kind, TokenKind::LessThan)
    assert_eq!(tokens[8].kind, TokenKind::LessThanEquals)
    assert_eq!(tokens[9].kind, TokenKind::GreaterThan)
    assert_eq!(tokens[10].kind, TokenKind::GreaterThanEquals)
    assert_eq!(tokens[11].kind, TokenKind::And)
    assert_eq!(tokens[12].kind, TokenKind::Or)
    assert_eq!(tokens[13].kind, TokenKind::Not)
    
    Ok(())
}

#[test]
fn test_keywords() -> Result<(), Error> {
    let source = "fn let if else while for return break continue struct enum trait impl"
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    assert_eq!(tokens[0].kind, TokenKind::Function)
    assert_eq!(tokens[1].kind, TokenKind::Let)
    assert_eq!(tokens[2].kind, TokenKind::If)
    assert_eq!(tokens[3].kind, TokenKind::Else)
    assert_eq!(tokens[4].kind, TokenKind::While)
    assert_eq!(tokens[5].kind, TokenKind::For)
    assert_eq!(tokens[6].kind, TokenKind::Return)
    assert_eq!(tokens[7].kind, TokenKind::Break)
    assert_eq!(tokens[8].kind, TokenKind::Continue)
    assert_eq!(tokens[9].kind, TokenKind::Struct)
    assert_eq!(tokens[10].kind, TokenKind::Enum)
    assert_eq!(tokens[11].kind, TokenKind::Trait)
    assert_eq!(tokens[12].kind, TokenKind::Impl)
    
    Ok(())
}

#[test]
fn test_nested_indentation() -> Result<(), Error> {
    let source = "
    fn outer() {
        if true {
            let x = 42
        }
    }
    "
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    // Count indentation levels
    let indent_count = tokens.iter()
        .filter(|t| t.kind == TokenKind::Indent)
        .count()
    let dedent_count = tokens.iter()
        .filter(|t| t.kind == TokenKind::Dedent)
        .count()
        
    assert_eq!(indent_count, 2)
    assert_eq!(dedent_count, 2)
    
    Ok(())
}

#[test]
fn test_numeric_literals() -> Result<(), Error> {
    let source = "42 3.14 0xFF 0b1010 1_000_000"
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    assert_eq!(tokens[0].kind, TokenKind::Integer)
    assert_eq!(tokens[0].value, "42")
    
    assert_eq!(tokens[1].kind, TokenKind::Float)
    assert_eq!(tokens[1].value, "3.14")
    
    assert_eq!(tokens[2].kind, TokenKind::Integer)
    assert_eq!(tokens[2].value, "0xFF")
    
    assert_eq!(tokens[3].kind, TokenKind::Integer)
    assert_eq!(tokens[3].value, "0b1010")
    
    assert_eq!(tokens[4].kind, TokenKind::Integer)
    assert_eq!(tokens[4].value, "1_000_000")
    
    Ok(())
}

#[test]
fn test_string_escapes() -> Result<(), Error> {
    let source = r#""\n\t\r\"\\\0""#
    let lexer = Lexer::new(source)
    let tokens = lexer.tokenize()?
    
    assert_eq!(tokens[0].kind, TokenKind::String)
    assert_eq!(tokens[0].value.len(), 6)
    
    Ok(())
}

#[test]
#[should_panic]
fn test_unterminated_string() {
    let source = r#"let msg = "Hello"#
    let lexer = Lexer::new(source)
    let _ = lexer.tokenize().unwrap()
}

#[test]
#[timeout(1)]
fn test_large_input() -> Result<(), Error> {
    let source = "x".repeat(1_000_000)
    let lexer = Lexer::new(&source)
    let tokens = lexer.tokenize()?
    
    assert_eq!(tokens.len(), 1)
    assert_eq!(tokens[0].kind, TokenKind::Identifier)
    assert_eq!(tokens[0].value.len(), 1_000_000)
    
    Ok(())
}
