# Seoggi Lexer Implementation
# Provides lexical analysis for Seoggi's indentation-based syntax

effect IO {
    fn read() -> String
    fn write(String)
}

effect Error {
    fn raise<T>(msg: String) -> T
}

# Token types for Seoggi's syntax
type TokenType {
    # Keywords
    Let | Fn | Type | Effect | Match | If | Else | For | While | Return |
    Linear | Region | Unsafe | Actor | Atomic | Spawn | Do | Async | Await |
    Import | Export | Module | Interface | Impl | As | Where | When |
    
    # Literals
    Int(Int) | Float(Float) | String(String) | Bool(Bool) | Char(Char) |
    
    # Identifiers
    Identifier(String) |
    
    # Operators
    Plus | Minus | Star | Slash | Percent |      # Arithmetic
    Eq | Neq | Lt | Gt | Leq | Geq |            # Comparison
    And | Or | Not |                             # Logical
    Arrow | FatArrow | Dot | Bang |             # Special
    PlusEq | MinusEq | StarEq | SlashEq |       # Compound assignment
    AndAnd | OrOr |                             # Short-circuit logical
    DotDot | DotDotEq |                         # Range operators
    Question | QuestionDot |                     # Optional chaining
    
    # Delimiters
    LParen | RParen | LBrace | RBrace |
    LBracket | RBracket | Comma | Colon |
    Dollar | BackTick |                         # For string interpolation
    
    # Indentation
    Indent | Dedent | Newline |
    
    # Special
    StringStart | StringPart | StringEnd |      # For string interpolation
    Comment(String) |                          # Preserve comments for documentation
    
    # End of file
    EOF
}

# Position tracking for error reporting
type Position {
    line: Int
    column: Int
    offset: Int
}

# Token with position information
type Token {
    type: TokenType
    pos: Position
    text: String  # Original text
}

# Lexer state
type Lexer {
    input: String
    pos: Position
    indent_stack: Vec<Int>
    current_indent: Int
    tokens: Vec<Token>
    in_string: Bool
}

# Create a new lexer instance
fn new_lexer(input: String) -> Lexer {
    Lexer {
        input,
        pos: Position { line: 1, column: 1, offset: 0 },
        indent_stack: vec![0],  # Base indentation level
        current_indent: 0,
        tokens: Vec::new(),
        in_string: false
    }
}

# Main lexing function
fn lex(input: String) -> IO + Error + Result<Vec<Token>> {
    let lexer = new_lexer(input)
    lexer.tokenize()
}

# Tokenize the entire input
fn tokenize(self: &mut Lexer) -> IO + Error + Result<Vec<Token>> {
    while !self.is_at_end() {
        match self.next_char() {
            ' ' | '\t' => self.handle_whitespace(),
            '\n' => self.handle_newline(),
            
            # Comments
            '#' => self.skip_comment(),
            
            # String literals
            '"' => self.lex_string(),
            '\'' => self.lex_char(),
            
            # Numbers
            '0'..='9' => self.lex_number(),
            
            # Identifiers and keywords
            'a'..='z' | 'A'..='Z' | '_' => self.lex_identifier(),
            
            # Operators
            '+' => self.add_token(TokenType::Plus),
            '-' => self.handle_dash(),
            '*' => self.add_token(TokenType::Star),
            '/' => self.add_token(TokenType::Slash),
            '%' => self.add_token(TokenType::Percent),
            
            '=' => self.handle_equals(),
            '!' => self.handle_bang(),
            '<' => self.handle_less(),
            '>' => self.handle_greater(),
            
            '&' => self.handle_and(),
            '|' => self.handle_or(),
            
            # Delimiters
            '(' => self.add_token(TokenType::LParen),
            ')' => self.add_token(TokenType::RParen),
            '{' => self.add_token(TokenType::LBrace),
            '}' => self.add_token(TokenType::RBrace),
            '[' => self.add_token(TokenType::LBracket),
            ']' => self.add_token(TokenType::RBracket),
            ',' => self.add_token(TokenType::Comma),
            ':' => self.add_token(TokenType::Colon),
            
            # Error for unexpected characters
            c => do raise(f"Unexpected character: {c} at {self.pos}")
        }
    }
    
    # Handle any remaining indentation
    self.handle_remaining_indentation()
    
    # Add EOF token
    self.add_token(TokenType::EOF)
    
    Ok(self.tokens)
}

# Helper functions for lexing specific token types

fn handle_whitespace(self: &mut Lexer) {
    let start_pos = self.pos
    let mut spaces = 0
    
    while self.peek() == ' ' || self.peek() == '\t' {
        if self.peek() == ' ' {
            spaces += 1
        } else {  # tab
            spaces += 4  # Convert tabs to spaces
        }
        self.advance()
    }
    
    if start_pos.column == 1 {  # Start of line
        self.handle_indentation(spaces)
    }
}

fn handle_indentation(self: &mut Lexer, spaces: Int) {
    if spaces > self.current_indent {
        # Indent
        self.indent_stack.push(spaces)
        self.current_indent = spaces
        self.add_token(TokenType::Indent)
    } else if spaces < self.current_indent {
        # Dedent (possibly multiple levels)
        while spaces < self.current_indent {
            self.indent_stack.pop()
            self.current_indent = *self.indent_stack.last().unwrap_or(&0)
            self.add_token(TokenType::Dedent)
        }
        
        if spaces != self.current_indent {
            do raise(f"Invalid indentation at {self.pos}")
        }
    }
}

fn lex_string(self: &mut Lexer) -> Error + Result<()> {
    let quote_type = self.peek()  # Can be ' or "
    self.advance()  # Skip opening quote
    let start = self.pos
    let mut value = String::new()
    let mut in_interpolation = false
    
    if quote_type == '"' {
        self.add_token(TokenType::StringStart)
    }
    
    while (self.peek() != quote_type || in_interpolation) && !self.is_at_end() {
        if self.peek() == '\\' {
            self.advance()
            value.push(match self.next_char() {
                'n' => '\n',
                't' => '\t',
                'r' => '\r',
                '"' => '"',
                '\'' => '\'',
                '\\' => '\\',
                '$' => '$',
                c => return Err(f"Invalid escape sequence: \\{c} at {self.pos}")
            })
        } else if self.peek() == '$' && self.peek_next() == '{' && quote_type == '"' {
            if !value.is_empty() {
                self.add_token(TokenType::String(value.clone()))
                value.clear()
            }
            self.advance()  # Skip $
            self.advance()  # Skip {
            self.add_token(TokenType::Dollar)
            self.add_token(TokenType::LBrace)
            in_interpolation = true
            
            // Recursively lex the interpolated expression
            self.lex_until_brace()?
            
            in_interpolation = false
            self.add_token(TokenType::StringPart)
        } else if self.peek() == '}' && in_interpolation {
            if !value.is_empty() {
                self.add_token(TokenType::String(value.clone()))
                value.clear()
            }
            self.advance()  # Skip }
            in_interpolation = false
        } else {
            value.push(self.next_char())
        }
    }
    
    if self.is_at_end() {
        return Err(f"Unterminated string starting at {start}")
    }
    
    if !value.is_empty() {
        self.add_token(TokenType::String(value))
    }
    
    self.advance()  # Skip closing quote
    
    if quote_type == '"' {
        self.add_token(TokenType::StringEnd)
    }
    
    Ok(())
}

fn lex_until_brace(&mut self) -> Error + Result<()> {
    let mut brace_count = 1
    
    while brace_count > 0 && !self.is_at_end() {
        match self.peek() {
            '{' => {
                brace_count += 1
                self.add_token(TokenType::LBrace)
                self.advance()
            }
            '}' => {
                brace_count -= 1
                if brace_count > 0 {
                    self.add_token(TokenType::RBrace)
                }
                self.advance()
            }
            '"' => {
                self.lex_string()?
            }
            _ => {
                self.tokenize_next()?
            }
        }
    }
    
    if brace_count > 0 {
        return Err(f"Unclosed interpolation at {self.pos}")
    }
    
    Ok(())
}

fn lex_number(self: &mut Lexer) -> Result<()> {
    let start = self.pos
    let mut value = String::new()
    let mut is_float = false
    
    while self.peek().is_digit() {
        value.push(self.next_char())
    }
    
    # Handle decimal point
    if self.peek() == '.' && self.peek_next().is_digit() {
        is_float = true
        value.push(self.next_char())  # Add decimal point
        
        while self.peek().is_digit() {
            value.push(self.next_char())
        }
    }
    
    # Handle exponent
    if self.peek().to_lowercase() == 'e' {
        is_float = true
        value.push(self.next_char())
        
        if self.peek() == '+' || self.peek() == '-' {
            value.push(self.next_char())
        }
        
        if !self.peek().is_digit() {
            return Err(f"Invalid number format at {self.pos}")
        }
        
        while self.peek().is_digit() {
            value.push(self.next_char())
        }
    }
    
    if is_float {
        self.add_token(TokenType::Float(value.parse().unwrap()))
    } else {
        self.add_token(TokenType::Int(value.parse().unwrap()))
    }
    
    Ok(())
}

fn lex_identifier(self: &mut Lexer) {
    let start = self.pos
    let mut name = String::new()
    
    while self.peek().is_alphanumeric() || self.peek() == '_' {
        name.push(self.next_char())
    }
    
    # Check for keywords
    let token_type = match name.as_str() {
        "let" => TokenType::Let,
        "fn" => TokenType::Fn,
        "type" => TokenType::Type,
        "effect" => TokenType::Effect,
        "match" => TokenType::Match,
        "if" => TokenType::If,
        "else" => TokenType::Else,
        "for" => TokenType::For,
        "while" => TokenType::While,
        "return" => TokenType::Return,
        "linear" => TokenType::Linear,
        "region" => TokenType::Region,
        "unsafe" => TokenType::Unsafe,
        "actor" => TokenType::Actor,
        "atomic" => TokenType::Atomic,
        "spawn" => TokenType::Spawn,
        "do" => TokenType::Do,
        "true" => TokenType::Bool(true),
        "false" => TokenType::Bool(false),
        "async" => TokenType::Async,
        "await" => TokenType::Await,
        "import" => TokenType::Import,
        "export" => TokenType::Export,
        "module" => TokenType::Module,
        "interface" => TokenType::Interface,
        "impl" => TokenType::Impl,
        "as" => TokenType::As,
        "where" => TokenType::Where,
        "when" => TokenType::When,
        _ => TokenType::Identifier(name)
    }
    
    self.add_token(token_type)
}

# Utility functions

fn is_at_end(self: &Lexer) -> Bool {
    self.pos.offset >= self.input.len()
}

fn peek(self: &Lexer) -> Char {
    if self.is_at_end() {
        '\0'
    } else {
        self.input[self.pos.offset]
    }
}

fn peek_next(self: &Lexer) -> Char {
    if self.pos.offset + 1 >= self.input.len() {
        '\0'
    } else {
        self.input[self.pos.offset + 1]
    }
}

fn advance(self: &mut Lexer) -> Char {
    let c = self.peek()
    self.pos.offset += 1
    
    if c == '\n' {
        self.pos.line += 1
        self.pos.column = 1
    } else {
        self.pos.column += 1
    }
    
    c
}

fn add_token(self: &mut Lexer, type: TokenType) {
    let text = self.input[self.token_start..self.pos.offset]
    self.tokens.push(Token {
        type,
        pos: self.token_start_pos,
        text
    })
}

# Error handling

fn error(pos: Position, message: String) -> Error {
    Error {
        pos,
        message,
        kind: ErrorKind::Lexical
    }
}

fn recover_from_error(&mut self) -> Result<()> {
    while !self.is_at_end() {
        if self.previous_token()?.type == TokenType::Newline {
            return Ok(())
        }
        
        match self.peek() {
            '\n' => {
                self.advance()
                self.add_token(TokenType::Newline)
                return Ok(())
            }
            '"' | '\'' => {
                if self.in_string {
                    self.in_string = false
                    self.advance()
                    return Ok(())
                }
            }
            _ => self.advance()
        }
    }
    Ok(())
}

# Test cases

#[test]
fn test_basic_lexing() {
    let input = "
    fn main() {
        let x = 42
        print(x)
    }
    "
    
    let tokens = lex(input).unwrap()
    assert_eq!(tokens[0].type, TokenType::Fn)
    assert_eq!(tokens[1].type, TokenType::Identifier("main"))
    assert_eq!(tokens[2].type, TokenType::LParen)
    assert_eq!(tokens[3].type, TokenType::RParen)
    assert_eq!(tokens[4].type, TokenType::LBrace)
    assert_eq!(tokens[5].type, TokenType::Indent)
    assert_eq!(tokens[6].type, TokenType::Let)
    assert_eq!(tokens[7].type, TokenType::Identifier("x"))
    assert_eq!(tokens[8].type, TokenType::Eq)
    assert_eq!(tokens[9].type, TokenType::Int(42))
}

#[test]
fn test_string_lexing() {
    let input = r#"let msg = "Hello, \"world\"!""#
    let tokens = lex(input).unwrap()
    assert_eq!(tokens[3].type, TokenType::String("Hello, \"world\"!"))
}

#[test]
fn test_indentation() {
    let input = "
    if true {
        print(1)
            print(2)
        print(3)
    }
    "
    
    let tokens = lex(input).unwrap()
    let indents = tokens.iter()
        .filter(|t| matches!(t.type, TokenType::Indent | TokenType::Dedent))
        .count()
    assert_eq!(indents, 4)  # 2 indents + 2 dedents
}

#[test]
fn test_string_interpolation() {
    let input = r#"let msg = "Hello, ${name}! Your score is ${score + 10}""#
    let lexer = new_lexer(input)
    let tokens = lexer.lex().unwrap()
    
    assert_eq!(tokens, vec![
        Token::new(TokenType::Let),
        Token::new(TokenType::Identifier("msg")),
        Token::new(TokenType::Eq),
        Token::new(TokenType::StringStart),
        Token::new(TokenType::String("Hello, ")),
        Token::new(TokenType::Dollar),
        Token::new(TokenType::LBrace),
        Token::new(TokenType::Identifier("name")),
        Token::new(TokenType::RBrace),
        Token::new(TokenType::StringPart),
        Token::new(TokenType::String("! Your score is ")),
        Token::new(TokenType::Dollar),
        Token::new(TokenType::LBrace),
        Token::new(TokenType::Identifier("score")),
        Token::new(TokenType::Plus),
        Token::new(TokenType::Int(10)),
        Token::new(TokenType::RBrace),
        Token::new(TokenType::StringEnd),
        Token::new(TokenType::EOF)
    ])
}

#[test]
fn test_indentation() {
    let input = r#"
if true {
    print("Hello")
        nested()
    back()
}
"#
    let lexer = new_lexer(input)
    let tokens = lexer.lex().unwrap()
    
    # Verify proper indentation tokens are generated
    assert!(tokens.iter().any(|t| t.type == TokenType::Indent))
    assert!(tokens.iter().any(|t| t.type == TokenType::Dedent))
}

#[test]
fn test_error_recovery() {
    let input = r#"
let x = 42
let y = @invalid
let z = 10
"#
    let lexer = new_lexer(input)
    let result = lexer.lex()
    
    # Should recover and continue lexing after error
    assert!(result.is_ok())
    let tokens = result.unwrap()
    assert!(tokens.iter().any(|t| matches!(t.type, TokenType::Identifier("z"))))
}

#[test]
fn test_nested_interpolation() {
    let input = r#"let msg = "Value: ${compute("${name}'s score: ${score}")}"#
    let lexer = new_lexer(input)
    let tokens = lexer.lex().unwrap()
    
    # Verify nested interpolation is handled correctly
    assert!(tokens.iter().filter(|t| t.type == TokenType::Dollar).count() == 3)
}

#[test]
fn test_compound_operators() {
    let input = "x += 1 && y -= 2"
    let lexer = new_lexer(input)
    let tokens = lexer.lex().unwrap()
    
    assert!(tokens.iter().any(|t| t.type == TokenType::PlusEq))
    assert!(tokens.iter().any(|t| t.type == TokenType::MinusEq))
    assert!(tokens.iter().any(|t| t.type == TokenType::AndAnd))
}
